{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNix4Ajv1Ea+jChcxvObpOZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SHDZ0gxMt-0L"},"outputs":[],"source":["!pip install git+https://github.com/konstmish/opt_methods.git"]},{"cell_type":"code","source":["!pip install -U ray"],"metadata":{"id":"6SuGYKi3uFKT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["HBM_Scaffnew and other Algorithms"],"metadata":{"id":"ccwyZl1zvqLW"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import ray\n","\n","###########################################\n","# Common settings and helper functions\n","###########################################\n","d = 5               # dimension of model\n","num_clients = 10     # number of clients\n","T_rounds = 300     # number of communication rounds\n","\n","# For our simulation we use quadratic losses: f_i(x) = 0.5*||x - b_i||^2.\n","# Its gradient is: grad f_i(x) = x - b_i.\n","np.random.seed(42)\n","client_b = [np.random.randn(d) for _ in range(num_clients)]\n","b_avg = np.mean(np.stack(client_b, axis=0), axis=0)\n","\n","def make_grad_func(b):\n","    \"\"\"Return a gradient function for a quadratic: grad f_i(x) = x - b.\"\"\"\n","    return lambda x: x - b\n","\n","def global_grad_norm(x_avg):\n","    \"\"\"Global gradient norm: ||x_avg - b_avg||.\"\"\"\n","    return np.linalg.norm(x_avg - b_avg)\n","\n","###########################################\n","# 1. Scaffold (Centralized simulation)\n","###########################################\n","class Scaffold:\n","    def __init__(self, client_grad_funcs, x0_clients, gamma):\n","        self.n = len(client_grad_funcs)\n","        self.grad_funcs = client_grad_funcs\n","        self.x = [x0.copy() for x0 in x0_clients]  # local models\n","        self.c = [np.zeros_like(x0_clients[0]) for _ in range(self.n)]\n","        self.c_server = np.mean(np.array(self.c), axis=0)\n","        self.gamma = gamma\n","\n","    def round(self):\n","        new_x = []\n","        for i in range(self.n):\n","            grad_i = self.grad_funcs[i](self.x[i])\n","            x_new = self.x[i] - self.gamma * (grad_i - self.c[i] + self.c_server)\n","            new_x.append(x_new)\n","        for i in range(self.n):\n","            grad_new = self.grad_funcs[i](new_x[i])\n","            grad_old = self.grad_funcs[i](self.x[i])\n","            self.c[i] = self.c[i] + (grad_new - grad_old)\n","        self.c_server = np.mean(np.array(self.c), axis=0)\n","        self.x = new_x\n","\n","    def run(self, rounds):\n","        trajectory = []\n","        for t in range(rounds):\n","            self.round()\n","            x_avg = np.mean(np.stack(self.x, axis=0), axis=0)\n","            trajectory.append(x_avg.copy())\n","        return trajectory\n","\n","###########################################\n","# 2. Scaffnew (ProxSkip for Federated Learning using Ray)\n","###########################################\n","@ray.remote\n","class ScaffnewClient:\n","    def __init__(self, x0, h0, grad_func):\n","        self.x = x0.copy()\n","        self.h = h0.copy()\n","        self.grad_func = grad_func\n","\n","    def local_update(self, gamma):\n","        grad = self.grad_func(self.x)\n","        x_hat = self.x - gamma * (grad - self.h)\n","        return x_hat\n","\n","    def apply_update(self, x_hat, x_comm, do_comm, gamma, p):\n","        if do_comm:\n","            new_x = x_comm\n","        else:\n","            new_x = x_hat\n","        self.h = self.h + (p / gamma) * (new_x - x_hat)\n","        self.x = new_x\n","\n","    def get_model(self):\n","        return self.x\n","\n","def run_scaffnew(T, gamma, p, d, client_b):\n","    clients = []\n","    x0 = np.zeros(d)\n","    h0 = np.zeros(d)\n","    for i in range(num_clients):\n","        grad_func = make_grad_func(client_b[i])\n","        client = ScaffnewClient.remote(x0, h0, grad_func)\n","        clients.append(client)\n","    traj = []\n","    for t in range(T):\n","        futures = [client.local_update.remote(gamma) for client in clients]\n","        x_hats = ray.get(futures)\n","        do_comm = random.random() < p\n","        if do_comm:\n","            x_comm = np.mean(np.stack(x_hats, axis=0), axis=0)\n","        else:\n","            x_comm = None\n","        update_futures = [client.apply_update.remote(x_hats[i], x_comm, do_comm, gamma, p)\n","                          for i, client in enumerate(clients)]\n","        ray.get(update_futures)\n","        models = ray.get([client.get_model.remote() for client in clients])\n","        x_avg = np.mean(np.stack(models, axis=0), axis=0)\n","        traj.append(x_avg.copy())\n","    return traj\n","\n","###########################################\n","# 3. FedLin (Federated Linearized Method using Ray)\n","###########################################\n","@ray.remote\n","class FedLinClient:\n","    def __init__(self, x0, grad_func, local_steps):\n","        self.x = x0.copy()\n","        self.grad_func = grad_func\n","        self.local_steps = local_steps\n","\n","    def local_update(self, gamma):\n","        for _ in range(self.local_steps):\n","            grad = self.grad_func(self.x)\n","            self.x = self.x - gamma * grad\n","        return self.x\n","\n","    def set_model(self, new_x):\n","        self.x = new_x.copy()\n","\n","    def get_model(self):\n","        return self.x\n","\n","def run_fedlin(T, gamma, local_steps, d, client_b):\n","    clients = []\n","    for i in range(num_clients):\n","        x0 = np.zeros(d)\n","        grad_func = make_grad_func(client_b[i])\n","        client = FedLinClient.remote(x0, grad_func, local_steps)\n","        clients.append(client)\n","    traj = []\n","    for t in range(T):\n","        futures = [client.local_update.remote(gamma) for client in clients]\n","        models = ray.get(futures)\n","        x_avg = np.mean(np.stack(models, axis=0), axis=0)\n","        update_futures = [client.set_model.remote(x_avg) for client in clients]\n","        ray.get(update_futures)\n","        traj.append(x_avg.copy())\n","    return traj\n","\n","###########################################\n","# 4. LocalGD (Standard local gradient descent using Ray)\n","###########################################\n","@ray.remote\n","class LocalGDClient:\n","    def __init__(self, x0, grad_func, local_steps):\n","        \"\"\"\n","        Args:\n","            x0 (np.ndarray): Initial model.\n","            grad_func: Function computing ∇fᵢ(x) for this client.\n","            local_steps (int): Number of local gradient steps per round.\n","        \"\"\"\n","        self.x = x0.copy()\n","        self.grad_func = grad_func\n","        self.local_steps = local_steps\n","\n","    def local_update(self, gamma):\n","        \"\"\"Perform local_steps of gradient descent without control variates.\"\"\"\n","        for _ in range(self.local_steps):\n","            grad = self.grad_func(self.x)\n","            self.x = self.x - gamma * grad\n","        return self.x\n","\n","    def set_model(self, new_x):\n","        self.x = new_x.copy()\n","\n","    def get_model(self):\n","        return self.x\n","\n","def run_local_gd(T, gamma, local_steps, d, client_b):\n","    \"\"\"\n","    Runs LocalGD for T rounds.\n","    Args:\n","        T: Number of rounds.\n","        gamma: Local stepsize.\n","        local_steps: Number of local steps per round.\n","        d: Model dimension.\n","        client_b: List of b vectors for each client.\n","    Returns:\n","        traj: List of aggregated models per round.\n","    \"\"\"\n","    clients = []\n","    for i in range(num_clients):\n","        x0 = np.zeros(d)\n","        grad_func = make_grad_func(client_b[i])\n","        client = LocalGDClient.remote(x0, grad_func, local_steps)\n","        clients.append(client)\n","    traj = []\n","    for t in range(T):\n","        futures = [client.local_update.remote(gamma) for client in clients]\n","        models = ray.get(futures)\n","        x_avg = np.mean(np.stack(models, axis=0), axis=0)\n","        update_futures = [client.set_model.remote(x_avg) for client in clients]\n","        ray.get(update_futures)\n","        traj.append(x_avg.copy())\n","    return traj\n","###########################################\n","# 5. Scaffnew with Heavy-Ball Momentum (Ray)\n","###########################################\n","@ray.remote\n","class ScaffnewMomentumClient:\n","    def __init__(self, x0, h0, grad_func, beta):\n","        self.x = x0.copy()\n","        self.x_prev = x0.copy()  # For momentum\n","        self.h = h0.copy()\n","        self.grad_func = grad_func\n","        self.beta = beta\n","\n","    def local_update(self, gamma):\n","        grad = self.grad_func(self.x)\n","        momentum = self.x - self.x_prev\n","        x_hat = self.x - gamma * (grad - self.h) + self.beta * momentum\n","        return x_hat\n","\n","    def apply_update(self, x_hat, x_comm, do_comm, gamma, p):\n","        if do_comm:\n","            new_x = x_comm\n","        else:\n","            new_x = x_hat\n","        self.h = self.h + (p / gamma) * (new_x - x_hat)\n","        self.x_prev = self.x.copy()  # Save current before updating\n","        self.x = new_x\n","\n","    def get_model(self):\n","        return self.x\n","\n","def run_scaffnew_momentum(T, gamma, p, beta, d, client_b):\n","    clients = []\n","    x0 = np.zeros(d)\n","    h0 = np.zeros(d)\n","    for i in range(num_clients):\n","        grad_func = make_grad_func(client_b[i])\n","        client = ScaffnewMomentumClient.remote(x0, h0, grad_func, beta)\n","        clients.append(client)\n","\n","    traj = []\n","    for t in range(T):\n","        x_hats = ray.get([client.local_update.remote(gamma) for client in clients])\n","        do_comm = random.random() < p\n","        if do_comm:\n","            x_comm = np.mean(np.stack(x_hats, axis=0), axis=0)\n","        else:\n","            x_comm = None\n","        ray.get([\n","            client.apply_update.remote(x_hats[i], x_comm, do_comm, gamma, p)\n","            for i, client in enumerate(clients)\n","        ])\n","        models = ray.get([client.get_model.remote() for client in clients])\n","        x_avg = np.mean(np.stack(models, axis=0), axis=0)\n","        traj.append(x_avg.copy())\n","    return traj\n","###########################################\n","# 6. Scaffold -momentum\n","###########################################\n","class ScaffoldMomentum:\n","    def __init__(self, client_grad_funcs, x0_clients, gamma, beta):\n","        self.n = len(client_grad_funcs)\n","        self.grad_funcs = client_grad_funcs\n","        self.x = [x0.copy() for x0 in x0_clients]               # local models\n","        self.x_prev = [x0.copy() for x0 in x0_clients]          # previous local models for momentum\n","        self.c = [np.zeros_like(x0_clients[0]) for _ in range(self.n)]\n","        self.c_server = np.mean(np.array(self.c), axis=0)\n","        self.gamma = gamma\n","        self.beta = beta\n","\n","    def round(self):\n","        new_x = []\n","        for i in range(self.n):\n","            grad_i = self.grad_funcs[i](self.x[i])\n","            momentum = self.x[i] - self.x_prev[i]\n","            x_new = self.x[i] - self.gamma * (grad_i - self.c[i] + self.c_server) + self.beta * momentum\n","            new_x.append(x_new)\n","\n","        for i in range(self.n):\n","            grad_new = self.grad_funcs[i](new_x[i])\n","            grad_old = self.grad_funcs[i](self.x[i])\n","            self.c[i] = self.c[i] + (grad_new - grad_old)\n","\n","        self.c_server = np.mean(np.array(self.c), axis=0)\n","\n","        # Update x_prev before overwriting x\n","        for i in range(self.n):\n","            self.x_prev[i] = self.x[i].copy()\n","        self.x = new_x\n","\n","    def run(self, rounds):\n","        trajectory = []\n","        for t in range(rounds):\n","            self.round()\n","            x_avg = np.mean(np.stack(self.x, axis=0), axis=0)\n","            trajectory.append(x_avg.copy())\n","        return trajectory\n","\n","\n","\n","\n","\n","###########################################\n","# Main: Run all algorithms and plot grad norm vs iteration\n","###########################################\n","# Parameters:\n","gamma_scaffold = 0.08\n","gamma_scaffnew = 0.08\n","p_scaffnew = 0.7\n","gamma_fedlin = 0.03\n","local_steps_fedlin = 1\n","gamma_localgd = 0.02\n","local_steps_localgd = 1\n","\n","# Run Scaffold (centralized simulation)\n","x0_clients = [np.zeros(d) for _ in range(num_clients)]\n","grad_funcs_scaffold = [make_grad_func(b) for b in client_b]\n","scaffold_solver = Scaffold(grad_funcs_scaffold, x0_clients, gamma_scaffold)\n","traj_scaffold = scaffold_solver.run(T_rounds)\n","grad_norms_scaffold = [global_grad_norm(x_avg) for x_avg in traj_scaffold]\n","\n","# Run Scaffnew:\n","traj_scaffnew = run_scaffnew(T_rounds, gamma_scaffnew, p_scaffnew, d, client_b)\n","grad_norms_scaffnew = [global_grad_norm(x_avg) for x_avg in traj_scaffnew]\n","\n","# Run FedLin:\n","traj_fedlin = run_fedlin(T_rounds, gamma_fedlin, local_steps_fedlin, d, client_b)\n","grad_norms_fedlin = [global_grad_norm(x_avg) for x_avg in traj_fedlin]\n","\n","# Run LocalGD:\n","traj_localgd = run_local_gd(T_rounds, gamma_localgd, local_steps_localgd, d, client_b)\n","grad_norms_localgd = [global_grad_norm(x_avg) for x_avg in traj_localgd]\n","\n","# Run Scaffnew-Momentum:\n","#beta_momentum = 0.4  # Momentum parameter\n","beta_momentum = 0.5  # Momentum parameter\n","traj_scaffnew_momentum = run_scaffnew_momentum(\n","    T_rounds, gamma_scaffnew, p_scaffnew, beta_momentum, d, client_b\n",")\n","grad_norms_scaffnew_momentum = [global_grad_norm(x_avg) for x_avg in traj_scaffnew_momentum]\n","\n","# Run Scaffold-Momentum:\n","#beta_momentum = 0.5  # Momentum parameter\n","#scaffold_momentum_solver = ScaffoldWithMomentum(\n"," #   client_grad_funcs=grad_funcs_scaffold,\n","  #  x0_clients=x0_clients,\n","   # gamma=gamma_scaffold,\n","    #beta=beta_momentum)\n","#traj_scaffold_momentum = scaffold_momentum_solver.run(T_rounds)\n","# Compute gradient norms over time\n","#grad_norms_scaffold_momentum = [global_grad_norm(x_avg) for x_avg in traj_scaffold_momentum]\n","\n","# Plot all curves including Scaffnew-Momentum:\n","iterations = list(range(T_rounds))\n","plt.figure(figsize=(10, 6))\n","plt.plot(iterations, grad_norms_scaffold, marker='o', label='Scaffold')\n","plt.plot(iterations, grad_norms_scaffnew, marker='s', label='Scaffnew')\n","plt.plot(iterations, grad_norms_scaffnew_momentum, marker='x', label='Scaffnew-Momentum')\n","plt.plot(iterations, grad_norms_fedlin, marker='^', label='FedLin')\n","plt.plot(iterations, grad_norms_localgd, marker='d', label='LocalGD')\n","plt.xlabel('Communication Rounds ')\n","plt.ylabel('Global Gradient Norm')\n","plt.title('Gradient Norm vs Communication Rounds for Scaffold, Scaffnew, Scaffnew-Momentum, FedLin, and LocalGD')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Shutdown Ray after all computations.\n","ray.shutdown()\n"],"metadata":{"id":"oIt-aWxCuH28"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Gradient norm convergence of different beta values"],"metadata":{"id":"z_YbNpznvle-"}},{"cell_type":"code","source":["\n","# Beta values in decreasing order\n","beta_values = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5]\n","scaffnew_momentum_results = {}\n","\n","# Run for each beta value\n","for beta in beta_values:\n","    print(f\"Running with beta = {beta}\")\n","    traj = run_scaffnew_momentum(\n","        T_rounds, gamma, p_scaffnew, beta, d, client_b\n","    )\n","    grad_norms = [global_grad_norm(x_avg) for x_avg in traj]\n","    scaffnew_momentum_results[beta] = grad_norms\n","\n","# Plotting\n","plt.figure(figsize=(10, 6))\n","iterations = list(range(T_rounds))\n","plt.plot(iterations, scaffnew_momentum_results)\n","\n","# Plot each beta's results\n","#for beta in beta_values:\n"," #   plt.plot(iterations, scaffnew_momentum_results[beta],\n","  #           label=f'β = {beta}')\n","\n","#plt.xlabel('Communication Rounds')\n","#plt.ylabel('Global Gradient Norm')\n","#plt.title('Scaffnew with Heavy-Ball Momentum for Different β Values')\n","#plt.legend()\n","#plt.grid(True)\n","#plt.show()"],"metadata":{"id":"WmsrHcZAuTL-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Gradient norm convergence of different beta values"],"metadata":{"id":"SrRsWz0pvZ_r"}},{"cell_type":"code","source":["# List of beta values to try\n","beta_values = [0.0, 0.2, 0.4, 0.6, 0.8]\n","scaffnew_momentum_results = {}\n","\n","# Run for each beta value\n","for beta in beta_values:\n","    traj = run_scaffnew_momentum(\n","        T_rounds, gamma_scaffnew, p_scaffnew, beta, d, client_b\n","    )\n","    grad_norms = [global_grad_norm(x_avg) for x_avg in traj]\n","    scaffnew_momentum_results[beta] = grad_norms\n","\n","# Plotting\n","plt.figure(figsize=(10, 6))\n","iterations = list(range(T_rounds))\n","\n","for beta, grad_norms in scaffnew_momentum_results.items():\n","    label = f'Scaffnew-Momentum (β={beta})'\n","    plt.plot(iterations, grad_norms, label=label)\n","\n","plt.xlabel('Communication Rounds ')\n","plt.ylabel('Global Gradient Norm ')\n","plt.title('Scaffnew with Heavy-Ball Momentum for Different β Values')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"vZxNDjCBuaM0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Addaptive beta HBM_Scaffnew"],"metadata":{"id":"iEA1QmeYvPJ_"}},{"cell_type":"code","source":["@ray.remote\n","class ScaffnewMomentumClient:\n","    def __init__(self, x0, h0, grad_func, beta):\n","        self.x = x0.copy()\n","        self.x_prev = x0.copy()\n","        self.h = h0.copy()\n","        self.grad_func = grad_func\n","        self.beta = beta\n","\n","    def local_update(self, gamma):\n","        grad = self.grad_func(self.x)\n","        momentum = self.x - self.x_prev\n","        x_hat = self.x - gamma * (grad - self.h) + self.beta * momentum\n","        return x_hat\n","\n","    def apply_update(self, x_hat, x_comm, do_comm, gamma, p):\n","        if do_comm:\n","            new_x = x_comm\n","        else:\n","            new_x = x_hat\n","        self.h = self.h + (p / gamma) * (new_x - x_hat)\n","        self.x_prev = self.x.copy()\n","        self.x = new_x\n","\n","    def get_model(self):\n","        return self.x\n","\n","    def set_beta(self, new_beta):\n","        self.beta = new_beta\n","\n","    def set_x(self, new_x):\n","        self.x = new_x.copy()\n","def run_scaffnew_momentum_adaptive(T, gamma, p, d, client_b):\n","    clients = []\n","    x0 = np.zeros(d)\n","    h0 = np.zeros(d)\n","    initial_beta = 1.0\n","    beta = initial_beta\n","    decay_rate = 0.065\n","    min_beta = 0.0\n","\n","    for i in range(num_clients):\n","        grad_func = make_grad_func(client_b[i])\n","        client = ScaffnewMomentumClient.remote(x0, h0, grad_func, beta)\n","        clients.append(client)\n","\n","    traj = []\n","    grad_norms = []\n","    beta_values = []\n","\n","    for t in range(T):\n","        # Save current state\n","        original_states = ray.get([client.get_model.remote() for client in clients])\n","\n","        # Try decayed beta\n","        beta_candidate = max(min_beta, beta - decay_rate)\n","        ray.get([client.set_beta.remote(beta_candidate) for client in clients])\n","\n","        x_hats_trial = ray.get([client.local_update.remote(gamma) for client in clients])\n","        do_comm_trial = random.random() < p\n","        x_comm_trial = np.mean(np.stack(x_hats_trial, axis=0), axis=0) if do_comm_trial else None\n","\n","        ray.get([\n","            client.apply_update.remote(x_hats_trial[i], x_comm_trial, do_comm_trial, gamma, p)\n","            for i, client in enumerate(clients)\n","        ])\n","        models_trial = ray.get([client.get_model.remote() for client in clients])\n","        x_avg_trial = np.mean(np.stack(models_trial, axis=0), axis=0)\n","        grad_trial = global_grad_norm(x_avg_trial)\n","\n","        if t == 0 or grad_trial < grad_norms[-1]:\n","            beta = beta_candidate\n","            traj.append(x_avg_trial.copy())\n","            grad_norms.append(grad_trial)\n","            beta_values.append(beta)\n","        else:\n","            # Revert to previous x and beta\n","            ray.get([client.set_beta.remote(beta) for client in clients])\n","            ray.get([client.set_x.remote(original_states[i]) for i, client in enumerate(clients)])\n","\n","            x_hats = ray.get([client.local_update.remote(gamma) for client in clients])\n","            do_comm = random.random() < p\n","            x_comm = np.mean(np.stack(x_hats, axis=0), axis=0) if do_comm else None\n","\n","            ray.get([\n","                client.apply_update.remote(x_hats[i], x_comm, do_comm, gamma, p)\n","                for i, client in enumerate(clients)\n","            ])\n","            models = ray.get([client.get_model.remote() for client in clients])\n","            x_avg = np.mean(np.stack(models, axis=0), axis=0)\n","            traj.append(x_avg.copy())\n","            grad_norms.append(global_grad_norm(x_avg))\n","            beta_values.append(beta)\n","\n","    return traj, grad_norms, beta_values\n","# Run the adaptive training\n","traj_adaptive, grad_norms_adaptive, beta_values = run_scaffnew_momentum_adaptive(\n","    T_rounds, gamma_scaffnew, p_scaffnew, d, client_b\n",")\n","\n","iterations = list(range(T_rounds))\n","\n","# Plot gradient norm and beta\n","plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(iterations, grad_norms_adaptive, marker='x', label='Gradient Norm')\n","plt.xlabel('Communication Rounds')\n","plt.ylabel('Global Gradient Norm')\n","plt.title('Gradient Norm vs Iteration')\n","plt.grid(True)\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(iterations, beta_values, marker='o', color='orange', label='Momentum β')\n","plt.xlabel('Communication Rounds')\n","plt.ylabel('β Value')\n","plt.title('Momentum Parameter (β) over Time')\n","plt.grid(True)\n","plt.legend()\n","\n","plt.suptitle('Scaffnew with Adaptive Heavy-Ball Momentum')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"AoEc9BObud8_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loss Curves for One beta and Adaptive Beta"],"metadata":{"id":"CCtLHQVrvHjv"}},{"cell_type":"code","source":["# Step 1: Run fixed-beta version (already done)\n","traj_fixed = run_scaffnew_momentum(\n","    T_rounds, gamma_scaffnew, p_scaffnew, beta_momentum, d, client_b\n",")\n","grad_norms_fixed = [global_grad_norm(x_avg) for x_avg in traj_fixed]\n","\n","# Step 2: Run adaptive-beta version\n","traj_adaptive, grad_norms_adaptive, beta_values = run_scaffnew_momentum_adaptive(\n","    T_rounds, gamma_scaffnew, p_scaffnew, d, client_b\n",")\n","\n","# Step 3: Plot both\n","iterations = list(range(T_rounds))\n","plt.figure(figsize=(10, 6))\n","\n","plt.plot(iterations, grad_norms_fixed, marker='o', label=f'Fixed β = {beta_momentum}')\n","plt.plot(iterations, grad_norms_adaptive, marker='x', label='Adaptive β (1→0)')\n","\n","plt.xlabel('Communication Rounds ')\n","plt.ylabel('Global Gradient Norm')\n","plt.title('Scaffnew with Heavy-Ball Momentum: Fixed vs Adaptive β')\n","plt.grid(True)\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"MvtTs-3lumKQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loss Curve for different Fixed beta and Adaptive Beta"],"metadata":{"id":"1r4sTTdVu1d0"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import ray\n","import urllib.request\n","import os\n","from sklearn.datasets import load_svmlight_file\n","from sklearn.metrics import log_loss\n","\n","#############################\n","# 1. Download & Load Dataset\n","#############################\n","def download_and_load_dataset(dataset_name=\"w8a\"):\n","    url = f\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/{dataset_name}\"\n","    if not os.path.exists(dataset_name):\n","        urllib.request.urlretrieve(url, dataset_name)\n","    X_sparse, y = load_svmlight_file(dataset_name)\n","    X = X_sparse.toarray()\n","    y = (y > 0).astype(int)  # Convert to binary {0,1}\n","    return X, y\n","\n","#############################\n","# 2. Gradient Function\n","#############################\n","def make_grad_func(Xb, yb):\n","    def grad_fn(w):\n","        preds = 1 / (1 + np.exp(-Xb @ w))\n","        grad = Xb.T @ (preds - yb) / len(yb)\n","        return grad\n","    return grad_fn\n","\n","#############################\n","# 3. Logistic Loss\n","#############################\n","def compute_logistic_loss(X, y, w):\n","    logits = X @ w\n","    preds = 1 / (1 + np.exp(-logits))\n","    return log_loss(y, preds)\n","\n","#############################\n","# 4. Ray Client\n","#############################\n","@ray.remote\n","class ScaffnewMomentumClient:\n","    def __init__(self, x0, h0, grad_func, beta):\n","        self.x = x0.copy()\n","        self.x_prev = x0.copy()\n","        self.h = h0.copy()\n","        self.grad_func = grad_func\n","        self.beta = beta\n","\n","    def local_update(self, gamma):\n","        grad = self.grad_func(self.x)\n","        momentum = self.x - self.x_prev\n","        x_hat = self.x - gamma * (grad - self.h) + self.beta * momentum\n","        return x_hat\n","\n","    def apply_update(self, x_hat, x_comm, do_comm, gamma, p):\n","        new_x = x_comm if do_comm else x_hat\n","        self.h = self.h + (p / gamma) * (new_x - x_hat)\n","        self.x_prev = self.x.copy()\n","        self.x = new_x\n","\n","    def get_model(self):\n","        return self.x\n","\n","    def set_beta(self, new_beta):\n","        self.beta = new_beta\n","\n","    def set_x(self, new_x):\n","        self.x = new_x.copy()\n","\n","#############################\n","# 5. Fixed Beta Run\n","#############################\n","def run_scaffnew_momentum(T, gamma, p, beta, d, client_b):\n","    clients = []\n","    x0 = np.zeros(d)\n","    h0 = np.zeros(d)\n","    for i in range(len(client_b)):\n","        Xb, yb = client_b[i]\n","        grad_func = make_grad_func(Xb, yb)\n","        client = ScaffnewMomentumClient.remote(x0, h0, grad_func, beta)\n","        clients.append(client)\n","\n","    traj = []\n","    for t in range(T):\n","        x_hats = ray.get([client.local_update.remote(gamma) for client in clients])\n","        do_comm = random.random() < p\n","        x_comm = np.mean(np.stack(x_hats, axis=0), axis=0) if do_comm else None\n","        ray.get([client.apply_update.remote(x_hats[i], x_comm, do_comm, gamma, p)\n","                 for i, client in enumerate(clients)])\n","        models = ray.get([client.get_model.remote() for client in clients])\n","        x_avg = np.mean(np.stack(models, axis=0), axis=0)\n","        traj.append(x_avg.copy())\n","    return traj\n","\n","#############################\n","# 6. Adaptive Beta Run\n","#############################\n","def run_scaffnew_momentum_adaptive(T, gamma, p, d, client_b):\n","    clients = []\n","    x0 = np.zeros(d)\n","    h0 = np.zeros(d)\n","    beta = 1.0\n","    decay_rate = 0.065\n","    min_beta = 0.0\n","\n","    for i in range(len(client_b)):\n","        Xb, yb = client_b[i]\n","        grad_func = make_grad_func(Xb, yb)\n","        client = ScaffnewMomentumClient.remote(x0, h0, grad_func, beta)\n","        clients.append(client)\n","\n","    traj, grad_norms, beta_values = [], [], []\n","\n","    for t in range(T):\n","        original_states = ray.get([client.get_model.remote() for client in clients])\n","\n","        beta_candidate = max(min_beta, beta - decay_rate)\n","        ray.get([client.set_beta.remote(beta_candidate) for client in clients])\n","\n","        x_hats_trial = ray.get([client.local_update.remote(gamma) for client in clients])\n","        do_comm_trial = random.random() < p\n","        x_comm_trial = np.mean(np.stack(x_hats_trial, axis=0), axis=0) if do_comm_trial else None\n","\n","        ray.get([client.apply_update.remote(x_hats_trial[i], x_comm_trial, do_comm_trial, gamma, p)\n","                 for i, client in enumerate(clients)])\n","        models_trial = ray.get([client.get_model.remote() for client in clients])\n","        x_avg_trial = np.mean(np.stack(models_trial, axis=0), axis=0)\n","        grad_trial = np.linalg.norm(x_avg_trial)\n","\n","        if t == 0 or grad_trial < grad_norms[-1]:\n","            beta = beta_candidate\n","            traj.append(x_avg_trial.copy())\n","            grad_norms.append(grad_trial)\n","            beta_values.append(beta)\n","        else:\n","            ray.get([client.set_beta.remote(beta) for client in clients])\n","            ray.get([client.set_x.remote(original_states[i]) for i, client in enumerate(clients)])\n","\n","            x_hats = ray.get([client.local_update.remote(gamma) for client in clients])\n","            do_comm = random.random() < p\n","            x_comm = np.mean(np.stack(x_hats, axis=0), axis=0) if do_comm else None\n","            ray.get([client.apply_update.remote(x_hats[i], x_comm, do_comm, gamma, p)\n","                     for i, client in enumerate(clients)])\n","            models = ray.get([client.get_model.remote() for client in clients])\n","            x_avg = np.mean(np.stack(models, axis=0), axis=0)\n","            traj.append(x_avg.copy())\n","            grad_norms.append(np.linalg.norm(x_avg))\n","            beta_values.append(beta)\n","\n","    return traj, grad_norms, beta_values\n","\n","#############################\n","# 7. Plot Logistic Loss\n","#############################\n","# Function to plot fixed vs adaptive beta loss curves\n","def plot_loss_vs_rounds(X, y, traj_fixed, traj_adaptive, betas_fixed=None):\n","    iterations = list(range(len(traj_fixed[0]) if isinstance(traj_fixed[0], list) else len(traj_fixed)))\n","    plt.figure(figsize=(10, 6))\n","\n","    # Plot fixed beta runs\n","    if isinstance(traj_fixed[0], list):  # Multiple beta runs\n","        for i, traj in enumerate(traj_fixed):\n","            losses = [compute_logistic_loss(X, y, w) for w in traj]\n","            label = f\"Fixed \\u03b2={betas_fixed[i]:.2f}\" if betas_fixed else f\"Fixed \\u03b2-{i}\"\n","            plt.plot(iterations, losses, label=label)\n","    else:  # Single fixed beta run\n","        losses_fixed = [compute_logistic_loss(X, y, w) for w in traj_fixed]\n","        plt.plot(iterations, losses_fixed, label=f\"Fixed \\u03b2={betas_fixed[0] if betas_fixed else 0.5}\")\n","\n","    # Plot adaptive\n","    losses_adaptive = [compute_logistic_loss(X, y, w) for w in traj_adaptive]\n","    plt.plot(iterations, losses_adaptive, label=\"Adaptive \\u03b2\", linestyle='--', linewidth=2)\n","\n","    plt.xlabel(\"Communication Rounds\")\n","    plt.ylabel(\"Average Logistic Loss\")\n","    plt.title(\"Loss vs Communication Rounds (Fixed vs Adaptive \\u03b2)\")\n","    plt.legend()\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.show()\n","\n","#############################\n","# 8. Run Everything\n","#############################\n","ray.init(ignore_reinit_error=True)\n","\n","X, y = download_and_load_dataset(\"w8a\")\n","d = X.shape[1]\n","num_clients = 10\n","T_rounds = 1000\n","p_scaffnew = 0.7\n","gamma_scaffnew = 0.08\n","betas = [0.1, 0.3, 0.5, 0.7, 0.9,1.0]\n","\n","client_b = []\n","idx_split = np.array_split(np.arange(X.shape[0]), num_clients)\n","for idx in idx_split:\n","    client_b.append((X[idx], y[idx]))\n","\n","# Run fixed beta versions\n","traj_fixed_all = []\n","for beta in betas:\n","    traj_fixed = run_scaffnew_momentum(T_rounds, gamma_scaffnew, p_scaffnew, beta, d, client_b)\n","    traj_fixed_all.append(traj_fixed)\n","\n","# Run adaptive beta version\n","traj_adaptive, _, _ = run_scaffnew_momentum_adaptive(T_rounds, gamma_scaffnew, p_scaffnew, d, client_b)\n","\n","# Plot\n","plot_loss_vs_rounds(X, y, traj_fixed_all, traj_adaptive, betas_fixed=betas)\n"],"metadata":{"id":"6alyrcUwuq6S"},"execution_count":null,"outputs":[]}]}